{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7032cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Epoch 01/60 | loss=1.7662 | val_f1_macro=0.3355 | best=0.3355 | per_class=[0.408 0.4   0.592 0.018 0.34  0.362 0.228]\n",
      "Epoch 02/60 | loss=1.1964 | val_f1_macro=0.6968 | best=0.6968 | per_class=[0.687 0.783 0.838 0.53  0.862 0.541 0.636]\n",
      "Epoch 03/60 | loss=0.5394 | val_f1_macro=0.7037 | best=0.7037 | per_class=[0.718 0.787 0.834 0.571 0.863 0.544 0.609]\n",
      "Epoch 04/60 | loss=0.3247 | val_f1_macro=0.7021 | best=0.7037 | per_class=[0.736 0.773 0.827 0.529 0.846 0.537 0.666]\n",
      "Epoch 05/60 | loss=0.2347 | val_f1_macro=0.6839 | best=0.7037 | per_class=[0.71  0.752 0.817 0.53  0.833 0.532 0.612]\n",
      "Epoch 06/60 | loss=0.1856 | val_f1_macro=0.6773 | best=0.7037 | per_class=[0.722 0.741 0.812 0.519 0.821 0.513 0.612]\n",
      "Epoch 07/60 | loss=0.1408 | val_f1_macro=0.6805 | best=0.7037 | per_class=[0.734 0.742 0.818 0.513 0.828 0.518 0.611]\n",
      "Epoch 08/60 | loss=0.1005 | val_f1_macro=0.6751 | best=0.7037 | per_class=[0.717 0.737 0.796 0.519 0.829 0.524 0.605]\n",
      "Epoch 09/60 | loss=0.0824 | val_f1_macro=0.6656 | best=0.7037 | per_class=[0.721 0.735 0.788 0.512 0.807 0.502 0.594]\n",
      "Epoch 10/60 | loss=0.0691 | val_f1_macro=0.6712 | best=0.7037 | per_class=[0.725 0.73  0.814 0.509 0.827 0.498 0.595]\n",
      "Epoch 11/60 | loss=0.0546 | val_f1_macro=0.6649 | best=0.7037 | per_class=[0.719 0.732 0.809 0.506 0.812 0.508 0.569]\n",
      "Epoch 12/60 | loss=0.0528 | val_f1_macro=0.6676 | best=0.7037 | per_class=[0.724 0.728 0.808 0.514 0.817 0.503 0.581]\n",
      "Epoch 13/60 | loss=0.0408 | val_f1_macro=0.6692 | best=0.7037 | per_class=[0.726 0.732 0.806 0.51  0.818 0.516 0.577]\n",
      "Epoch 14/60 | loss=0.0369 | val_f1_macro=0.6727 | best=0.7037 | per_class=[0.729 0.735 0.807 0.509 0.825 0.506 0.598]\n",
      "Epoch 15/60 | loss=0.0332 | val_f1_macro=0.6655 | best=0.7037 | per_class=[0.726 0.733 0.8   0.506 0.808 0.501 0.585]\n",
      "Epoch 16/60 | loss=0.0287 | val_f1_macro=0.6667 | best=0.7037 | per_class=[0.726 0.728 0.8   0.502 0.817 0.508 0.586]\n",
      "Epoch 17/60 | loss=0.0208 | val_f1_macro=0.6690 | best=0.7037 | per_class=[0.713 0.738 0.798 0.514 0.815 0.523 0.581]\n",
      "Epoch 18/60 | loss=0.0206 | val_f1_macro=0.6712 | best=0.7037 | per_class=[0.721 0.732 0.809 0.514 0.807 0.517 0.599]\n",
      "Epoch 19/60 | loss=0.0190 | val_f1_macro=0.6660 | best=0.7037 | per_class=[0.723 0.732 0.803 0.506 0.809 0.516 0.573]\n",
      "Epoch 20/60 | loss=0.0156 | val_f1_macro=0.6688 | best=0.7037 | per_class=[0.728 0.74  0.8   0.511 0.811 0.504 0.587]\n",
      "Epoch 21/60 | loss=0.0127 | val_f1_macro=0.6686 | best=0.7037 | per_class=[0.726 0.729 0.791 0.507 0.814 0.508 0.605]\n",
      "Epoch 22/60 | loss=0.0111 | val_f1_macro=0.6673 | best=0.7037 | per_class=[0.725 0.739 0.799 0.513 0.803 0.515 0.576]\n",
      "Epoch 23/60 | loss=0.0115 | val_f1_macro=0.6734 | best=0.7037 | per_class=[0.728 0.726 0.806 0.52  0.812 0.519 0.603]\n",
      "Epoch 24/60 | loss=0.0079 | val_f1_macro=0.6701 | best=0.7037 | per_class=[0.731 0.731 0.799 0.517 0.808 0.507 0.597]\n",
      "Epoch 25/60 | loss=0.0077 | val_f1_macro=0.6666 | best=0.7037 | per_class=[0.732 0.733 0.804 0.511 0.802 0.503 0.581]\n",
      "Epoch 26/60 | loss=0.0054 | val_f1_macro=0.6661 | best=0.7037 | per_class=[0.731 0.723 0.804 0.508 0.813 0.492 0.592]\n",
      "Epoch 27/60 | loss=0.0047 | val_f1_macro=0.6683 | best=0.7037 | per_class=[0.727 0.731 0.799 0.51  0.813 0.505 0.594]\n",
      "Epoch 28/60 | loss=0.0037 | val_f1_macro=0.6681 | best=0.7037 | per_class=[0.735 0.73  0.805 0.506 0.81  0.503 0.587]\n",
      "Epoch 29/60 | loss=0.0031 | val_f1_macro=0.6686 | best=0.7037 | per_class=[0.724 0.737 0.807 0.509 0.805 0.51  0.588]\n",
      "Epoch 30/60 | loss=0.0027 | val_f1_macro=0.6681 | best=0.7037 | per_class=[0.731 0.728 0.807 0.51  0.813 0.504 0.584]\n",
      "Epoch 31/60 | loss=0.0023 | val_f1_macro=0.6684 | best=0.7037 | per_class=[0.729 0.73  0.802 0.512 0.812 0.504 0.591]\n",
      "Epoch 32/60 | loss=0.0016 | val_f1_macro=0.6686 | best=0.7037 | per_class=[0.722 0.728 0.803 0.509 0.813 0.512 0.594]\n",
      "Epoch 33/60 | loss=0.0015 | val_f1_macro=0.6673 | best=0.7037 | per_class=[0.726 0.726 0.805 0.508 0.812 0.507 0.587]\n",
      "Epoch 34/60 | loss=0.0013 | val_f1_macro=0.6692 | best=0.7037 | per_class=[0.73  0.731 0.807 0.511 0.814 0.507 0.584]\n",
      "Epoch 35/60 | loss=0.0012 | val_f1_macro=0.6666 | best=0.7037 | per_class=[0.726 0.726 0.803 0.509 0.813 0.508 0.581]\n",
      "Epoch 36/60 | loss=0.0012 | val_f1_macro=0.6678 | best=0.7037 | per_class=[0.732 0.731 0.805 0.506 0.811 0.507 0.583]\n",
      "Epoch 37/60 | loss=0.0011 | val_f1_macro=0.6682 | best=0.7037 | per_class=[0.731 0.729 0.806 0.508 0.812 0.507 0.585]\n",
      "Epoch 38/60 | loss=0.0012 | val_f1_macro=0.6679 | best=0.7037 | per_class=[0.727 0.73  0.805 0.507 0.812 0.512 0.583]\n",
      "Epoch 39/60 | loss=0.0012 | val_f1_macro=0.6667 | best=0.7037 | per_class=[0.73  0.725 0.803 0.509 0.813 0.502 0.584]\n",
      "Epoch 40/60 | loss=0.0012 | val_f1_macro=0.6682 | best=0.7037 | per_class=[0.731 0.728 0.805 0.506 0.813 0.508 0.585]\n",
      "Epoch 41/60 | loss=0.0010 | val_f1_macro=0.6664 | best=0.7037 | per_class=[0.729 0.728 0.801 0.505 0.811 0.505 0.585]\n",
      "Epoch 42/60 | loss=0.0010 | val_f1_macro=0.6673 | best=0.7037 | per_class=[0.731 0.729 0.803 0.505 0.809 0.505 0.589]\n",
      "Epoch 43/60 | loss=0.0008 | val_f1_macro=0.6674 | best=0.7037 | per_class=[0.729 0.73  0.802 0.504 0.809 0.508 0.589]\n",
      "Epoch 44/60 | loss=0.0010 | val_f1_macro=0.6674 | best=0.7037 | per_class=[0.729 0.729 0.804 0.51  0.81  0.504 0.585]\n",
      "Epoch 45/60 | loss=0.0008 | val_f1_macro=0.6664 | best=0.7037 | per_class=[0.727 0.728 0.803 0.51  0.808 0.504 0.585]\n",
      "Epoch 46/60 | loss=0.0007 | val_f1_macro=0.6666 | best=0.7037 | per_class=[0.729 0.729 0.802 0.508 0.809 0.505 0.585]\n",
      "Epoch 47/60 | loss=0.0007 | val_f1_macro=0.6654 | best=0.7037 | per_class=[0.727 0.729 0.801 0.506 0.809 0.503 0.581]\n",
      "Epoch 48/60 | loss=0.0007 | val_f1_macro=0.6661 | best=0.7037 | per_class=[0.73  0.729 0.801 0.507 0.81  0.504 0.582]\n",
      "Epoch 49/60 | loss=0.0007 | val_f1_macro=0.6659 | best=0.7037 | per_class=[0.73  0.728 0.801 0.507 0.81  0.503 0.582]\n",
      "Epoch 50/60 | loss=0.0008 | val_f1_macro=0.6663 | best=0.7037 | per_class=[0.729 0.728 0.801 0.507 0.811 0.504 0.584]\n",
      "Epoch 51/60 | loss=0.0006 | val_f1_macro=0.6665 | best=0.7037 | per_class=[0.729 0.728 0.802 0.509 0.81  0.503 0.585]\n",
      "Epoch 52/60 | loss=0.0006 | val_f1_macro=0.6665 | best=0.7037 | per_class=[0.729 0.728 0.802 0.509 0.81  0.505 0.583]\n",
      "Epoch 53/60 | loss=0.0006 | val_f1_macro=0.6663 | best=0.7037 | per_class=[0.728 0.728 0.802 0.508 0.811 0.504 0.583]\n",
      "Epoch 54/60 | loss=0.0006 | val_f1_macro=0.6661 | best=0.7037 | per_class=[0.729 0.729 0.802 0.508 0.81  0.504 0.581]\n",
      "Epoch 55/60 | loss=0.0006 | val_f1_macro=0.6666 | best=0.7037 | per_class=[0.729 0.729 0.802 0.509 0.81  0.505 0.582]\n",
      "Epoch 56/60 | loss=0.0006 | val_f1_macro=0.6664 | best=0.7037 | per_class=[0.728 0.729 0.802 0.508 0.81  0.505 0.583]\n",
      "Epoch 57/60 | loss=0.0006 | val_f1_macro=0.6664 | best=0.7037 | per_class=[0.728 0.729 0.802 0.508 0.81  0.505 0.583]\n",
      "Epoch 58/60 | loss=0.0006 | val_f1_macro=0.6664 | best=0.7037 | per_class=[0.728 0.729 0.802 0.507 0.81  0.505 0.584]\n",
      "Epoch 59/60 | loss=0.0005 | val_f1_macro=0.6665 | best=0.7037 | per_class=[0.728 0.729 0.802 0.508 0.81  0.505 0.583]\n",
      "Epoch 60/60 | loss=0.0006 | val_f1_macro=0.6665 | best=0.7037 | per_class=[0.728 0.729 0.802 0.508 0.81  0.505 0.583]\n",
      "\n",
      "Best val_f1_macro=0.7037\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8284    0.6336    0.7181      4550\n",
      "           1     0.7987    0.7758    0.7871      2061\n",
      "           2     0.8148    0.8538    0.8339      2169\n",
      "           3     0.6728    0.4954    0.5706      1859\n",
      "           4     0.8134    0.9181    0.8626      1685\n",
      "           5     0.4817    0.6260    0.5445      2420\n",
      "           6     0.4575    0.9102    0.6089       579\n",
      "\n",
      "    accuracy                         0.7077     15323\n",
      "   macro avg     0.6953    0.7447    0.7037     15323\n",
      "weighted avg     0.7332    0.7077    0.7102     15323\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.config import *\n",
    "from src.utils import load_data\n",
    "from src.preprocessing import *\n",
    "from src.nn import *  # CSRDataset, csr_collate\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Model\n",
    "# ------------------------------------------------------------\n",
    "class SparseDeepMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Sparse input -> dense hidden via torch.sparse.mm, then deeper dense stack.\n",
    "    AMP OFF (sparse.mm on CUDA doesn't support fp16 addmm_sparse_cuda).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim: int, hidden: int, num_classes: int, dropout: float = 0.30):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Parameter(torch.empty(hidden, in_dim))\n",
    "        self.b1 = nn.Parameter(torch.zeros(hidden))\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden, hidden)\n",
    "        self.ln2 = nn.LayerNorm(hidden)\n",
    "\n",
    "        self.fc3 = nn.Linear(hidden, hidden)\n",
    "        self.ln3 = nn.LayerNorm(hidden)\n",
    "\n",
    "        self.fc4 = nn.Linear(hidden, hidden)\n",
    "        self.ln4 = nn.LayerNorm(hidden)\n",
    "\n",
    "        self.out = nn.Linear(hidden, num_classes)\n",
    "\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "        nn.init.kaiming_uniform_(self.W1, a=np.sqrt(5))\n",
    "\n",
    "    def forward(self, X_sparse):\n",
    "        if X_sparse.is_cuda and X_sparse.dtype != torch.float32:\n",
    "            X_sparse = X_sparse.float()\n",
    "\n",
    "        h = torch.sparse.mm(X_sparse, self.W1.t()) + self.b1\n",
    "        h = self.act(h)\n",
    "        h = self.drop(h)\n",
    "\n",
    "        h = self.fc2(h); h = self.ln2(h); h = self.act(h); h = self.drop(h)\n",
    "        h = self.fc3(h); h = self.ln3(h); h = self.act(h); h = self.drop(h)\n",
    "        h = self.fc4(h); h = self.ln4(h); h = self.act(h); h = self.drop(h)\n",
    "\n",
    "        return self.out(h)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# SoftF1 (macro) + CE (optional, aligns training to macro-F1)\n",
    "# ------------------------------------------------------------\n",
    "class SoftF1Loss(nn.Module):\n",
    "    def __init__(self, num_classes: int, eps: float = 1e-9):\n",
    "        super().__init__()\n",
    "        self.C = num_classes\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        # logits: (B,C), y: (B,)\n",
    "        p = torch.softmax(logits, dim=1)\n",
    "        y_one = torch.nn.functional.one_hot(y, num_classes=self.C).float()\n",
    "\n",
    "        tp = (p * y_one).sum(dim=0)\n",
    "        fp = (p * (1 - y_one)).sum(dim=0)\n",
    "        fn = ((1 - p) * y_one).sum(dim=0)\n",
    "\n",
    "        soft_f1 = (2 * tp + self.eps) / (2 * tp + fp + fn + self.eps)\n",
    "        return 1.0 - soft_f1.mean()\n",
    "\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, ce: nn.Module, softf1: nn.Module, alpha: float = 0.8):\n",
    "        super().__init__()\n",
    "        self.ce = ce\n",
    "        self.softf1 = softf1\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, logits, y):\n",
    "        return self.alpha * self.ce(logits, y) + (1.0 - self.alpha) * self.softf1(logits, y)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Utils: predict\n",
    "# ----------------------------\n",
    "def predict_proba_csr(model: nn.Module, X_csr: sp.csr_matrix, device, batch_size=4096,\n",
    "                     log_priors: torch.Tensor | None = None, tau: float = 0.0):\n",
    "    model.eval()\n",
    "    dummy_y = np.zeros(X_csr.shape[0], dtype=np.int64)\n",
    "    ds = CSRDataset(X_csr, dummy_y)\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=False,\n",
    "                    collate_fn=lambda b: csr_collate(b, device=device))\n",
    "\n",
    "    probs = []\n",
    "    with torch.no_grad():\n",
    "        for Xb, _ in dl:\n",
    "            Xb = Xb.float()\n",
    "            logits = model(Xb)\n",
    "            if log_priors is not None and tau != 0.0:\n",
    "                logits = logits + tau * log_priors\n",
    "            probs.append(torch.softmax(logits, dim=1).detach().cpu().numpy())\n",
    "    return np.vstack(probs)\n",
    "\n",
    "\n",
    "def predict_labels_from_df(model: nn.Module, preprocess, X_df, device, batch_size=4096,\n",
    "                           log_priors: torch.Tensor | None = None, tau: float = 0.0):\n",
    "    X_csr = preprocess.transform(X_df)\n",
    "    proba = predict_proba_csr(model, X_csr, device=device, batch_size=batch_size, log_priors=log_priors, tau=tau)\n",
    "    pred = proba.argmax(axis=1)\n",
    "    return pred, proba\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Main\n",
    "# ----------------------------\n",
    "news_df = load_data(DEVELOPMENT_PATH)\n",
    "X = news_df.drop(columns=[\"y\"])\n",
    "y = news_df[\"y\"]\n",
    "\n",
    "X_train_df, X_val_df, y_train_s, y_val_s = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=SEED\n",
    ")\n",
    "\n",
    "y_train_np = y_train_s.to_numpy(dtype=np.int64)\n",
    "y_val_np   = y_val_s.to_numpy(dtype=np.int64)\n",
    "\n",
    "preprocess = build_preprocess(\"nn\")\n",
    "X_train_csr = preprocess.fit_transform(X_train_df)\n",
    "X_val_csr   = preprocess.transform(X_val_df)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "train_ds = CSRDataset(X_train_csr, y_train_np)\n",
    "val_ds   = CSRDataset(X_val_csr,   y_val_np)\n",
    "\n",
    "# ---- memory-safe training (accumulation) ----\n",
    "batch_size_train = 256\n",
    "accum_steps = 4  # effective batch = 1024\n",
    "train_dl = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=batch_size_train,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda b: csr_collate(b, device=device),\n",
    ")\n",
    "val_dl = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda b: csr_collate(b, device=device),\n",
    ")\n",
    "\n",
    "in_dim = X_train_csr.shape[1]\n",
    "num_classes = 7\n",
    "u = np.unique(y_train_np)\n",
    "assert u.min() == 0 and u.max() == num_classes - 1 and len(u) == num_classes, u\n",
    "\n",
    "# ---- class weights (clipped) ----\n",
    "counts = np.bincount(y_train_np, minlength=num_classes)\n",
    "class_weights = counts.sum() / (num_classes * np.maximum(counts, 1))\n",
    "class_weights = np.clip(class_weights, 0.5, 5.0)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32, device=device)\n",
    "\n",
    "# ---- logit adjustment ----\n",
    "priors = counts / counts.sum()\n",
    "log_priors = torch.log(torch.tensor(priors, device=device, dtype=torch.float32) + 1e-12)\n",
    "tau = 0.7  # 0.5 / 0.7 / 1.0 (0.7 spesso piÃ¹ stabile di 1.0)\n",
    "\n",
    "# ---- model ----\n",
    "hidden = 1536\n",
    "dropout = 0.30  # more regularization (you were overfitting)\n",
    "model = SparseDeepMLP(in_dim=in_dim, hidden=hidden, num_classes=num_classes, dropout=dropout).to(device)\n",
    "\n",
    "# ---- optimizer + schedule (less aggressive) ----\n",
    "epochs = 60\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=5e-4)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=3e-4,\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=max(1, len(train_dl) // accum_steps),\n",
    "    pct_start=0.10,\n",
    "    div_factor=40.0,\n",
    "    final_div_factor=300.0,\n",
    ")\n",
    "\n",
    "# ---- loss aligned to macro-F1 ----\n",
    "USE_SOFTF1 = True\n",
    "if USE_SOFTF1:\n",
    "    ce = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    softf1 = SoftF1Loss(num_classes=num_classes)\n",
    "    loss_fn = CombinedLoss(ce, softf1, alpha=0.85)  # closer to CE but nudges macro-F1\n",
    "else:\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "best_f1 = -1.0\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    for step, (Xb, yb) in enumerate(train_dl, start=1):\n",
    "        Xb = Xb.float()\n",
    "\n",
    "        logits = model(Xb)\n",
    "        logits = logits + tau * log_priors\n",
    "\n",
    "        loss = loss_fn(logits, yb) / accum_steps\n",
    "        loss.backward()\n",
    "        total_loss += loss.item() * accum_steps\n",
    "\n",
    "        if step % accum_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    if len(train_dl) % accum_steps != 0:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # ---- validation ----\n",
    "    model.eval()\n",
    "    all_pred, all_true = [], []\n",
    "    with torch.no_grad():\n",
    "        for Xb, yb in val_dl:\n",
    "            Xb = Xb.float()\n",
    "            logits = model(Xb)\n",
    "            logits = logits + tau * log_priors\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "            all_pred.append(pred.detach().cpu().numpy())\n",
    "            all_true.append(yb.detach().cpu().numpy())\n",
    "\n",
    "    all_pred = np.concatenate(all_pred)\n",
    "    all_true = np.concatenate(all_true)\n",
    "\n",
    "    f1m = f1_score(all_true, all_pred, average=\"macro\")\n",
    "    f1_per_class = f1_score(all_true, all_pred, average=None)\n",
    "    avg_loss = total_loss / max(1, len(train_dl))\n",
    "\n",
    "    if f1m > best_f1:\n",
    "        best_f1 = f1m\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d}/{epochs} | loss={avg_loss:.4f} | \"\n",
    "        f\"val_f1_macro={f1m:.4f} | best={best_f1:.4f} | \"\n",
    "        f\"per_class={np.round(f1_per_class, 3)}\"\n",
    "    )\n",
    "\n",
    "# restore best\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "print(f\"\\nBest val_f1_macro={best_f1:.4f}\\n\")\n",
    "\n",
    "# More detailed report on best model (optional)\n",
    "with torch.no_grad():\n",
    "    proba_val = predict_proba_csr(model, X_val_csr, device=device, batch_size=4096,\n",
    "                                 log_priors=log_priors, tau=tau)\n",
    "pred_val = proba_val.argmax(axis=1)\n",
    "print(classification_report(y_val_np, pred_val, digits=4))\n",
    "\n",
    "# ----------------------------\n",
    "# Predict on another dataset\n",
    "# ----------------------------\n",
    "# other_df = load_data(OTHER_PATH)\n",
    "# X_other = other_df.drop(columns=[\"y\"], errors=\"ignore\")\n",
    "# pred_other, proba_other = predict_labels_from_df(\n",
    "#     model, preprocess, X_other, device=device, batch_size=4096,\n",
    "#     log_priors=log_priors, tau=tau\n",
    "# )\n",
    "# print(pred_other[:20], proba_other.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b124a4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 2 5 5 5 2 3 2 4 5 0 1 4 2 5 2 1 5 0 6] (20000, 7)\n"
     ]
    }
   ],
   "source": [
    "X_test = pd.read_csv(EVALUATION_PATH, index_col=0, na_values='\\\\N')\n",
    "X_test = initial_prep(X_test, False)\n",
    "X_other = X_test\n",
    "pred_other, proba_other = predict_labels_from_df(\n",
    "    model, preprocess, X_other, device=device, batch_size=4096,\n",
    "    log_priors=log_priors, tau=tau\n",
    ")\n",
    "print(pred_other[:20], proba_other.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2a54825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 0, ..., 0, 5, 0], shape=(76611,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18e52fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame(\n",
    "{\n",
    "    \"Id\": range(0,20000),\n",
    "    \"Predicted\": pred_other\n",
    "},\n",
    "index=range(0,20000)\n",
    ")\n",
    "submission_df.to_csv('prova.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "news_clf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
